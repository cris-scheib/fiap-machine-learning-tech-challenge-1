# API de Livros - FIAP Machine Learning Tech Challenge 1

Projeto de extraÃ§Ã£o e API pÃºblica para consulta de livros, integrando web scraping e FastAPI.

| ![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg) ![FastAPI](https://img.shields.io/badge/framework-FastAPI-009688?logo=fastapi) ![MIT License](https://img.shields.io/badge/license-MIT-yellow.svg) |
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|

-----------------------------------

## SumÃ¡rio

- [DescriÃ§Ã£o](#descriÃ§Ã£o)
- [Tecnologias Utilizadas](#tecnologias-utilizadas)
- [Arquitetura](#arquitetura)
- [Como Utilizar](#como-utilizar)
- [Endpoints](#endpoints)
- [LicenÃ§a, Autores e Agradecimentos](#licenÃ§a-autores)

-----------------------------------

## DescriÃ§Ã£o

O objetivo deste projeto Ã© expor uma **API RESTful** para facilitar o acesso aos dados de diversos livros. Esses dados originalmente sÃ£o extraÃ­dos via **web scraping** do site [Books to Scrape](https://books.toscrape.com/) 

Os dados disponÃ­veis envolvem informaÃ§Ãµes sobre:

- `Id`: Identificador do livro
- `TÃ­tulo`: TÃ­tulo do livro
- `PreÃ§o`: PreÃ§o do livro
- `Disponibilidade`: Status da disponibilidade do livro
- `AvaliaÃ§Ã£o`: AvaliaÃ§Ã£o do livro em estrelas
- `Categoria`: Categoria do livro
- `Imagem`: Link da imagem do livro


-----------------------------------

## Tecnologias Utilizadas

- **Python 3.11**
- **FastAPI**
- **Uvicorn**
- **BeautifulSoup4**
- **SQLite**

-----------------------------------

## Arquitetura

O projeto segue uma arquitetura em camadas, inspirada no Clean Architecture,
separando responsabilidades em diferentes mÃ³dulos e 
facilitando manutenÃ§Ã£o e testes.

### ğŸ“‚ Estrutura do RepositÃ³rio

```
fiap-machine-learning-tech-challenge-1/
â”œâ”€â”€ api/                         # AplicaÃ§Ã£o FastAPI
â”‚   â”œâ”€â”€ main.py                  # Entrypoint (uvicorn app.main:app)
â”‚   â”œâ”€â”€ requirements.txt         # DependÃªncias da API
â”‚   â””â”€â”€ app/                     # CÃ³digo-fonte da API
â”‚       â”œâ”€â”€ controllers/         # ImplementaÃ§Ã£o dos endpoints (routes)
â”‚       â”œâ”€â”€ core/                # ConfiguraÃ§Ãµes de autenticaÃ§Ã£o e sessÃ£o de BD
â”‚       â”œâ”€â”€ models/              # Modelos SQLAlchemy
â”‚       â”œâ”€â”€ schemas/             # Schemas Pydantic (request/response)
â”‚       â””â”€â”€ services/            # LÃ³gica de negÃ³cio e componente de scraping
â”œâ”€â”€ requirements.txt             # DependÃªncias gerais do projeto
â”œâ”€â”€ runtime.txt                  # VersÃ£o do runtime (deploy)
â””â”€â”€ vercel.json                  # ConfiguraÃ§Ãµes de deploy
```

### DescriÃ§Ã£o das camadas:

**controllers**: Define os pontos de entrada da API (endpoints) e realiza roteamento.

**services**: Implementa a lÃ³gica de negÃ³cio, orquestrando operaÃ§Ãµes de scraping e acesso a dados via models.

**models**: Representa o domÃ­nio de dados, definindo entidades e mapeamentos com SQLAlchemy.

**schemas**: ContÃ©m os Pydantic models para validaÃ§Ã£o e serializaÃ§Ã£o de requisiÃ§Ãµes e respostas.

**core**: Agrupa configuraÃ§Ãµes centrais, como autenticaÃ§Ã£o JWT, inicializaÃ§Ã£o de sessÃ£o de banco de dados e configuraÃ§Ãµes gerais.

Essa separaÃ§Ã£o melhora a modularidade, favorece testes unitÃ¡rios e permite evoluir cada camada independentemente.

-----------------------------------

## Como Utilizar

VocÃª pode usar a API de duas formas: **localmente** no seu ambiente de desenvolvimento ou 
consumindo a **versÃ£o jÃ¡ deployada**.

Para sua conveniÃªncia, o repositÃ³rio jÃ¡ inclui um banco de dados (`.sqlite`) e um arquivo (`.csv`) 
com os dados atualizados obtidos via Web Scraping previamente, alÃ©m de um usuÃ¡rio de testes jÃ¡ criado. 
Isso permite que vocÃª teste a API imediatamente com uma base de dados de cerca de mil livros, sem precisar executar o Web Scraping.

**AutenticaÃ§Ã£o (vÃ¡lido para ambos os modos)**

Cadastre um usuÃ¡rio (ou utilize o de teste)

UsuÃ¡rio de teste:

    username: test_user  
    password: test12345

NÃ£o se esqueÃ§a de gerar e usar o token JWT antes de acessar os dados.

### â˜ï¸ Via Deploy (produÃ§Ã£o)

Acesse a versÃ£o pÃºblica em: https://fiap-machine-learning-tech-challeng-taupe.vercel.app/api/docs 

LÃ¡ vocÃª terÃ¡ o Swagger UI e poderÃ¡ testar todos os endpoints diretamente no navegador.

### ğŸ  ExecuÃ§Ã£o Local

### PrÃ©-requisitos

- Git
- Python 3.11+

### Passos

1. **Clone o repositÃ³rio**
   ```bash
   git clone https://github.com/cris-scheib/fiap-machine-learning-tech-challenge-1.git
   cd fiap-machine-learning-tech-challenge-1
   ```
2. **Crie e ative um ambiente virtual**
   ```bash
   python -m venv venv
   source venv/bin/activate   # Linux/macOS
   venv\Scripts\activate    # Windows
   ```
3. **Instale dependÃªncias gerais**
   ```bash
   pip install -r requirements.txt
   ```
4. **Inicie a API**
   ```bash
   cd api
   uvicorn main:app --reload
   ```
5. **Execute o Scraping (Opcional)**

    > **AtenÃ§Ã£o:** O processo de Web Scraping Ã© demorado, levando mais de 30 minutos. 
    > **Ele nÃ£o Ã© necessÃ¡rio para iniciar a API**, a menos que vocÃª queira gerar os dados do zero,e **antes de sua
    execuÃ§Ã£o certifique-se de criar ou ter um usuÃ¡rio criado**.

   ```bash
   cd api
   python -m app.services.scrapper.scrapper_service
   ```

A API estarÃ¡ disponÃ­vel em `http://127.0.0.1:8000`. 

A documentaÃ§Ã£o interativa Ã© acessÃ­vel em `http://127.0.0.1:8000/docs` (Swagger UI) 
e `http://127.0.0.1:8000/redoc` (ReDoc).

## Endpoints

### AutenticaÃ§Ã£o (opcional)

#### `POST /api/v1/auth/login`
- **DescriÃ§Ã£o:** Gera um token JWT para acesso a rotas protegidas.
- **Body Request:**
  ```json
  { "username": "seu_usuario", "password": "sua_senha" }
  ```
- **Resposta (200):**
  ```json
  { "access_token": "<seu_token>", "token_type": "bearer" }
  ```

Inclua no header das requisiÃ§Ãµes protegidas:
```
Authorization: Bearer <seu_token>
```

---

### `GET /api/v1/health`
- **DescriÃ§Ã£o:** Verifica se a API estÃ¡ no ar.
- **Resposta (200):**
  ```json
  { "status": "ok" }
  ```

---

### `GET /api/v1/books`
- **DescriÃ§Ã£o:** Lista todos os livros carregados.
- **Resposta (200):**
  ```json
  [
    { "id": 1, "title": "A Light in the Attic", "price": 51.77, "category": "Travel", "availability": "In stock", "rating": 3 },
    ...
  ]
  ```

---

### `GET /api/v1/books/{id}`
- **DescriÃ§Ã£o:** ObtÃ©m detalhes completos de um livro pelo seu ID.
- **Path Param:**
  - `id` (int)
- **Resposta (200):**
  ```json
  { "id": 1, "title": "A Light in the Attic", "price": 51.77, "category": "Travel", "availability": "In stock", "rating": 3, "description": "DescriÃ§Ã£o detalhada..." }
  ```
- **Resposta (404):**
  ```json
  { "detail": "Book not found" }
  ```

---

### `GET /api/v1/books/search`
- **DescriÃ§Ã£o:** Busca livros por tÃ­tulo parcial e/ou categoria.
- **Query Params:**
  - `title` (string, opcional)
  - `category` (string, opcional)
- **Resposta (200):** Lista de livros que atendem aos filtros.

---

### `GET /api/v1/categories`
- **DescriÃ§Ã£o:** Retorna todas as categorias existentes.
- **Resposta (200):**
  ```json
  ["Travel", "Mystery", "Historical Fiction", ...]
  ```

## LicenÃ§a, Autores

### ğŸ§‘â€ğŸ’» Desenvolvido por

- `Beatriz Rosa Carneiro Gomes - RM365967`
- `Cristine Scheibler - RM365433`
- `Guilherme Fernandes Dellatin - RM365508`
- `Iana Alexandre Neri - RM360484`
- `JoÃ£o Lucas Oliveira Hilario - RM366185`

Este projeto Ã© apenas para fins educacionais e segue a licenÃ§a MIT.


---------------- Lembrete para remover abaixo ----------------
### Features

- **Complete Data Extraction**: Title, price, rating, availability, category, image URL
- **Robust Error Handling**: Retry logic and comprehensive logging
- **Respectful Scraping**: Configurable delays and rate limiting
- **Pagination Support**: Automatically handles multi-page categories
- **Data Quality**: Text cleaning and normalization

### Output

The scraper creates a CSV file in the `data/` directory with the following columns:
- `title`: Book title
- `price`: Numeric price value
- `rating`: Star rating (One, Two, Three, Four, Five)
- `availability`: Availability status
- `category`: Book category
- `image_url`: Full URL to the book cover image
- `book_url`: URL to the book detail page

### Configuration

You can modify the following in `scripts/scrapper.py`:
- `DELAY_BETWEEN_REQUESTS`: Delay between requests (default: 1.0 seconds)
- `MAX_RETRIES`: Maximum retry attempts (default: 3)

## API <a name="api"></a>

### Quick Start

1. Create and activate a Python virtual environment:
```bash
python3 -m venv venv
source venv/bin/activate  # Linux/macOS
# or
venv\Scripts\activate  # Windows
```

2. Install dependencies:
```bash
# Option 1: Using pip directly
pip install -r requirements.txt

# Option 2: Using the installation script
python install_dependencies.py
```

> **Note**: If you encounter import errors, make sure to run the installation script to properly install all dependencies.

3. Run the script to start the server:
```bash
# Option 1: Standard startup
python start_server.py

# Option 2: Startup with dependency checks (recommended if you have import errors)
python start_server_with_checks.py
```

4. Or navigate to the API directory and run:
```bash
cd api
uvicorn main:app --reload
```

### Available Endpoints

- `GET /api/v1/books`: List all books
- `GET /api/v1/books/search?title=&category=`: Search books by title and category
- `GET /api/v1/books/{id}`: Get a book by ID

### Endpoint GET /api/v1/books/{id}

This endpoint returns the details of a specific book by its ID.

**Example Response (Book found):**
```json
{
  "id": 1,
  "title": "A Light in the Attic",
  "price": 51.77,
  "availability": "In stock",
  "rating": "Three",
  "category": "Poetry"
}
```

**Example Response (Book not found):**
```json
{
  "detail": "Error Book not found."
}
```

### Compatibility Issues

If you encounter compatibility issues when running the API, try:

1. Using Python 3.9 or 3.10 instead of newer versions
2. Running the `simulate_endpoint.py` script to test functionality without starting the server
3. Checking if the dependency versions in `requirements.txt` are compatible with your Python version

## Project Structure <a name="structure"></a>

```
fiap-machine-learning-tech-challenge-1/
â”œâ”€â”€ api/                    # REST API application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ controllers/    # API controllers
â”‚   â”‚   â”œâ”€â”€ core/          # Core functionality (auth, database)
â”‚   â”‚   â”œâ”€â”€ models/        # Data models
â”‚   â”‚   â”œâ”€â”€ schemas/       # Pydantic schemas
â”‚   â”‚   â””â”€â”€ services/      # Business logic
â”‚   â”œâ”€â”€ main.py            # FastAPI application entry point
â”‚   â””â”€â”€ requirements.txt   # API dependencies
â”œâ”€â”€ scripts/               # Web scraping tools
â”‚   â”œâ”€â”€ scrapper.py        # Main scraper script
â”‚   â”œâ”€â”€ demo_scrapper.py   # Demo version for testing
â”‚   â”œâ”€â”€ test_scrapper.py   # Test script
â”‚   â”œâ”€â”€ utils.py           # Utility functions
â”‚   â”œâ”€â”€ requirements.txt   # Scraper dependencies
â”‚   â”œâ”€â”€ run_scrapper.bat   # Windows batch file
â”‚   â””â”€â”€ README.md          # Scraper documentation
â”œâ”€â”€ data/                  # Output directory for scraped data
â””â”€â”€ README.md              # This file
```

![image]()

## Troubleshooting

### Import Errors

If you encounter import errors like `Unable to import 'fastapi'` or `Unable to import 'sqlalchemy'`, follow these steps:

1. Make sure you have activated your virtual environment
2. Run the environment check script to diagnose and fix issues:
   ```bash
   python check_environment.py
   ```
   This script will check your Python version, installed dependencies, and project structure, and offer to install any missing dependencies.

3. Alternatively, run the installation script to install all dependencies:
   ```bash
   python install_dependencies.py
   ```

4. If the issue persists, try installing the dependencies manually:
   ```bash
   pip install fastapi==0.95.2 uvicorn==0.22.0 pydantic==1.10.8 sqlalchemy==1.4.48 beautifulsoup4==4.12.2 requests==2.31.0
   ```

## Compatibility Issues

This project was developed and tested with Python 3.8+. Some compatibility issues may occur with older versions.

## Licensing, Authors and Acknowledgments<a name="licensing"></a>
