# Books to Scrape Web Scraper

-----------------------------------

## Componente de Scraping

O cora√ß√£o da coleta de dados do projeto reside no componente de scraping, projetado para ser robusto e eficiente na extra√ß√£o de informa√ß√µes do site `books.toscrape.com`. Ele √© dividido em duas partes principais: um servi√ßo orquestrador (**`scrapper_service.py`**) e um m√≥dulo de utilit√°rios (**`scrapper_utils.py`**), que trabalham em conjunto.

### Fluxo de Execu√ß√£o do Scraper

O processo de scraping √© orquestrado pelo `scrapper_service.py` e segue os seguintes passos:

1.  **Busca de Categorias**: O scraper primeiro acessa a p√°gina inicial do site para mapear e extrair as URLs de todas as categorias de livros dispon√≠veis.
2.  **Navega√ß√£o por Categoria**: Para cada categoria encontrada, ele navega pela sua p√°gina inicial.
3.  **Coleta de Livros e Pagina√ß√£o**: O scraper extrai a URL de cada livro listado na p√°gina e, de forma inteligente, procura por um bot√£o "next" para avan√ßar para as pr√≥ximas p√°ginas da mesma categoria, garantindo que todos os livros sejam coletados.
4.  **Extra√ß√£o de Dados**: Para cada URL de livro, uma requisi√ß√£o √© feita para extrair os dados brutos da p√°gina (t√≠tulo, pre√ßo, avalia√ß√£o, etc.).
5.  **Limpeza e Processamento**: Os dados brutos extra√≠dos s√£o processados e limpos pelas fun√ß√µes do m√≥dulo `scrapper_utils.py`.
6.  **Armazenamento**: Ao final do processo, os dados limpos e estruturados s√£o salvos em dois locais:
    * No banco de dados **SQLite**, para serem consumidos pela API.
    * Em um arquivo **CSV**, servindo para an√°lises futuras.

-----------------------------------

### A Classe `BooksToScrapeScraper`

Esta classe encapsula toda a l√≥gica e o estado do scraper. Seus m√©todos s√£o organizados para dividir o processo complexo de scraping em etapas menores e gerenci√°veis.

- **__init__(self, ...)**: O construtor da classe. Inicializa o scraper com a URL base do site e o `delay` (atraso) a ser respeitado entre as requisi√ß√µes. Ele tamb√©m valida a URL para garantir que o processo n√£o inicie com um endere√ßo inv√°lido.


- **get_all_category_urls(self)**: Respons√°vel por acessar a p√°gina inicial do site, encontrar o menu lateral de categorias e extrair o nome e a URL de cada uma delas, preparando a lista de "tarefas" para o scraper.


- **get_all_book_urls_from_category(category_url)**: M√©todo est√°tico que recebe a URL de uma categoria e navega por todas as suas p√°ginas (utilizando a l√≥gica de pagina√ß√£o "next") para coletar as URLs de todos os livros contidos nela.


- **extract_book_data(book_url)**: Recebe a URL de um √∫nico livro e extrai todas as informa√ß√µes detalhadas de sua p√°gina: t√≠tulo, pre√ßo, avalia√ß√£o, disponibilidade, categoria e link da imagem. Utiliza intensivamente as fun√ß√µes do `scrapper_utils.py` para limpar e formatar os dados.


- **scrape_all_books(self)**: √â o m√©todo orquestrador principal. Ele executa o fluxo completo: chama `get_all_category_urls`, itera sobre os resultados, chama `get_all_book_urls_from_category` para cada categoria, e por fim, `extract_book_data` para cada livro, respeitando o `delay` definido entre as requisi√ß√µes.


- **save_to_csv(books_data, ...)**: Um m√©todo est√°tico que recebe a lista final de dados dos livros e utiliza a biblioteca `pandas` para salv√°-los de forma organizada em um arquivo CSV com nome √∫nico (gerado com timestamp).


- **save_to_db(books_data, db)**: Tamb√©m est√°tico, este m√©todo √© respons√°vel por persistir os dados no banco de dados. Ele converte cada dicion√°rio de livro em uma entidade SQLAlchemy (`Book`) e realiza uma opera√ß√£o de inser√ß√£o em massa (`add_all`) para maior efici√™ncia.

### Utilit√°rios de Scraping (`scrapper_utils.py`)

Este m√≥dulo cont√©m um conjunto de fun√ß√µes auxiliares de baixo n√≠vel, respons√°veis pelo trabalho pesado de realizar requisi√ß√µes, processar e extrair dados brutos.

- **safe_request(url, ...)**: Realiza requisi√ß√µes HTTP de forma segura e resiliente. Inclui um `User-Agent` para simular um navegador, m√∫ltiplas tentativas (retries) com delay em caso de falha e timeouts para evitar que o script fique travado.


- **clean_text(text)**: Limpa strings removendo espa√ßos em branco extras, tabula√ß√µes e quebras de linha, padronizando o texto para armazenamento.


- **extract_price(price_text)**: Utiliza express√µes regulares (regex) para localizar e extrair um valor num√©rico (`float`) a partir de uma string de pre√ßo (ex: "¬£51.77"), ignorando s√≠mbolos de moeda.


- **extract_rating(rating_class)**: Extrai a avalia√ß√£o em texto (ex: 'Five', 'Three') a partir do atributo `class` do elemento HTML da classifica√ß√£o por estrelas.


- **check_availability(availability_text)**: Verifica e padroniza o status de disponibilidade do livro para "In Stock" ou "Out of Stock", garantindo consist√™ncia dos dados.


- **validate_url(url)**: Uma fun√ß√£o simples para verificar se uma string de URL possui um formato v√°lido antes de tentar fazer uma requisi√ß√£o.


- **create_filename(base_name, ...)**: Gera um nome de arquivo √∫nico adicionando um timestamp (data e hora), ideal para salvar os arquivos CSV sem sobrescrever os dados de coletas anteriores.

## Example Output

```
INFO:__main__:Initializing the database and creating tables if necessary...
INFO:__main__:Database ready.
INFO:__main__:Starting Books to Scrape scraper...
INFO:__main__:Starting to scrape all books...
INFO:__main__:Fetching category URLs...
....
INFO:__main__:Successfully extracted data for: 1,000 Places to See Before You Die
...
INFO:__main__:Scraping completed. Total books extracted: 11
INFO:__main__:Inserted 11 records into the database.
INFO:__main__:Total records: 11
INFO:__main__:Data Summary:
INFO:__main__:- Total books: 11
INFO:__main__:- Categories: 1
INFO:__main__:- Price range: ¬£23.21 - ¬£56.88
INFO:__main__:- Average price: ¬£39.79
INFO:__main__:- Rating distribution: {'Two': 3, 'Three': 3, 'Four': 2, 'One': 2, 'Five': 1}
INFO:__main__:Scraping completed successfully!
INFO:__main__:Data saved to db: 11
INFO:__main__:Scraping completed successfully!
INFO:__main__:Data saved to: ../api/app/core/data\books_data_20250806_115921.csv
```


## Licen√ßa e Autores

### üßë‚Äçüíª Desenvolvido por

- `Beatriz Rosa Carneiro Gomes - RM365967`
- `Cristine Scheibler - RM365433`
- `Guilherme Fernandes Dellatin - RM365508`
- `Iana Alexandre Neri - RM360484`
- `Jo√£o Lucas Oliveira Hilario - RM366185`

Este projeto √© apenas para fins educacionais e segue a licen√ßa MIT.